# update_keywords.py
# ì˜¤ëŠ˜ì˜ í‚¤ì›Œë“œ 20ê°œ ìˆ˜ì§‘ â†’ keywords.csvë¥¼ "ê·¸ í•œ ì¤„ë§Œ" ë‚¨ê¸°ë„ë¡ ì™„ì „ ë®ì–´ì“°ê¸°
# ì†ŒìŠ¤: NewsAPI, Naver News API, Google News RSS (í‚¤ ì—†ìœ¼ë©´ ê°€ëŠ¥í•œ ì†ŒìŠ¤ë§Œ ì‚¬ìš©)

import os, re, random, requests, xml.etree.ElementTree as ET
from urllib.parse import quote

CSV = os.getenv("KEYWORDS_CSV", "keywords.csv")
NEWSAPI_KEY = os.getenv("NEWSAPI_KEY", "").strip()
NAVER_ID = os.getenv("NAVER_CLIENT_ID", "").strip()
NAVER_SECRET = os.getenv("NAVER_CLIENT_SECRET", "").strip()

# -------------------- ìˆ˜ì§‘ê¸° --------------------
def fetch_newsapi_titles(api_key: str, page_size=50):
    if not api_key:
        return []
    try:
        url = "https://newsapi.org/v2/top-headlines"
        params = {"country": "kr", "pageSize": page_size, "language": "ko"}
        r = requests.get(url, params=params, headers={"X-Api-Key": api_key}, timeout=20)
        r.raise_for_status()
        data = r.json()
        return [a.get("title", "") for a in data.get("articles", []) if a.get("title")]
    except Exception:
        return []

def fetch_naver_titles(client_id: str, client_secret: str, per_query=15):
    if not (client_id and client_secret):
        return []
    queries = ["ì˜¤ëŠ˜", "ì†ë³´", "ê²½ì œ", "ì •ì±…", "ê¸°ìˆ ", "ì‹ ì œí’ˆ", "ë¦¬ë·°", "ë¶„ì„", "ì´ìŠˆ"]
    out = []
    for q in queries:
        try:
            url = f"https://openapi.naver.com/v1/search/news.json?query={quote(q)}&display={per_query}&sort=sim"
            headers = {"X-Naver-Client-Id": client_id, "X-Naver-Client-Secret": client_secret}
            r = requests.get(url, headers=headers, timeout=20)
            r.raise_for_status()
            data = r.json()
            for it in data.get("items", []):
                t = it.get("title", "")
                if t: out.append(t)
        except Exception:
            continue
    return out

def fetch_google_rss_titles():
    try:
        url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"
        r = requests.get(url, timeout=20)
        r.raise_for_status()
        root = ET.fromstring(r.text)
        titles = []
        for item in root.findall(".//item"):
            t = item.findtext("title")
            if t: titles.append(t)
        return titles[:100]
    except Exception:
        return []

# -------------------- í‚¤ì›Œë“œ í›„ë³´ ì¶”ì¶œ --------------------
STOPWORDS = set("""
ì†ë³´ ë‹¨ë… ì˜ìƒ í¬í†  ì¸í„°ë·° ì „ë¬¸ ê¸°ì ë„¤í‹°ì¦Œ ì¢…í•© í˜„ì¥ ë‹¨ì‹ 
ì˜¤ëŠ˜ ë‚´ì¼ ì–´ì œ ì´ë²ˆ ì§€ë‚œ ê´€ë ¨ ë°œí‘œ ê³µê°œ í™•ì¸ ì „ë§ ê³µì‹ ì‚¬ì‹¤
""".split())

def normalize_title(t: str) -> str:
    t = re.sub(r"\s*[-â€“â€”]\s*[^-â€“â€”]+$", "", t)  # ' - ë§¤ì²´ëª…' ì œê±°
    t = re.sub(r"\[[^\]]+\]|\([^)]+\)|ã€[^ã€‘]+ã€‘|<[^>]+>", " ", t)  # ê´„í˜¸/íƒœê·¸ ì œê±°
    t = re.sub(r"[â€œâ€\"'â€™â€˜Â·|Â·â€¢â€¢â–¶â–·â–²â–³â–¼â–½â—†â—‡â˜…â˜†â€¦~!?:;]", " ", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t

def extract_phrases_ko(title: str):
    t = normalize_title(title)
    toks = [w for w in t.split() if 2 <= len(w) <= 12 and w not in STOPWORDS]
    cands = set()
    # 2~4ê·¸ë¨ ìš°ì„ 
    for n in (4, 3, 2):
        if len(toks) < n: continue
        for i in range(len(toks)-n+1):
            p = " ".join(toks[i:i+n])
            if len(p.replace(" ", "")) < 4: continue
            cands.add(p)
    # 1ê·¸ë¨ ë³´ì¶©
    for w in toks:
        if 2 <= len(w) <= 10:
            cands.add(w)
    return list(cands)

def rank_and_pick(phrases, k=20):
    # ë¹ˆë„ + ê¸¸ì´ ê°€ì¤‘ì¹˜
    freq = {}
    for p in phrases:
        freq[p] = freq.get(p, 0) + 1
    scored = []
    for p, c in freq.items():
        L = len(p.replace(" ", ""))
        score = c * 10 + min(L, 12)
        scored.append((score, p))
    scored.sort(reverse=True)
    pool = [p for _, p in scored[:80]]
    random.shuffle(pool)
    out, seen = [], set()
    for p in pool:
        base = p.replace(" ", "")
        if base in seen: continue
        seen.add(base)
        out.append(p)
        if len(out) >= k: break
    return out

# -------------------- CSV ì“°ê¸°(ì™„ì „ ë®ì–´ì“°ê¸°) --------------------
def write_today_keywords_only(keywords):
    """
    keywords.csvë¥¼ ì˜¤ëŠ˜ í‚¤ì›Œë“œ 1ì¤„ë§Œ ë‚¨ê¸°ë„ë¡ 'ì™„ì „íˆ ë®ì–´ì“´ë‹¤'.
    ê¸°ì¡´ ì¤„ì€ ëª¨ë‘ ì‚­ì œë¨.
    """
    new_first = ", ".join(keywords)
    with open(CSV, "w", encoding="utf-8", newline="\n") as f:
        f.write(new_first + "\n")
    print("[OK] wrote ONLY today's 20 keywords (file truncated):")
    print(new_first)

# -------------------- ë©”ì¸ --------------------
SEED_BACKUP = [
    "ê²½ì œ ë™í–¥", "ì£¼ì‹ ì‹œì¥", "í™˜ìœ¨ ì „ë§", "ë¶€ë™ì‚° ì •ì±…", "ì „ê¸°ì°¨ ë°°í„°ë¦¬",
    "ìŠ¤ë§ˆíŠ¸í° ì‹ ì œí’ˆ", "AI íŠ¸ë Œë“œ", "í´ë¼ìš°ë“œ ë³´ì•ˆ", "ì›ìì¬ ê°€ê²©", "ë°˜ë„ì²´ ìˆ˜ìš”",
    "ì†Œë¹„ì ë¬¼ê°€", "ê´€ê´‘ ì‚°ì—…", "ìš°ì£¼ íƒì‚¬", "í—¬ìŠ¤ì¼€ì–´ ì›¨ì–´ëŸ¬ë¸”", "ì¹œí™˜ê²½ ì—ë„ˆì§€",
    "í•´ì™¸ ì§êµ¬", "ì—…ë¬´ ìë™í™”", "ìƒì‚°ì„± ë„êµ¬", "ë””ì§€í„¸ ë§ˆì¼€íŒ…", "ë¬´ë£Œ ì†Œí”„íŠ¸ì›¨ì–´"
]

def main():
    titles = []
    titles += fetch_newsapi_titles(NEWSAPI_KEY, page_size=50)
    titles += fetch_naver_titles(NAVER_ID, NAVER_SECRET, per_query=15)
    titles += fetch_google_rss_titles()

    phrases = []
    for t in titles:
        phrases += extract_phrases_ko(t)

    # ì†ŒìŠ¤ê°€ ë¹„ê±°ë‚˜ ë¶€ì¡±í•˜ë©´ ì‹œë“œë¡œ ë³´ì¶©
    if len(set(phrases)) < 20:
        phrases += SEED_BACKUP

    picked = rank_and_pick(phrases, k=20)
    # ì•ˆì „ì¥ì¹˜: ìµœì†Œ 2ê°œëŠ” í™•ë³´
    if len(picked) < 2:
        picked = (SEED_BACKUP + picked)[:20]

    # ğŸ”¥ ì™„ì „ ë®ì–´ì“°ê¸°
    write_today_keywords_only(picked)

if __name__ == "__main__":
    main()
