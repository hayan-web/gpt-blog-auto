# -*- coding: utf-8 -*-
"""
update_keywords.py
- ë‰´ìŠ¤/ê²€ìƒ‰ íƒ€ì´í‹€ì„ ëª¨ì•„ 2~4ë‹¨ì–´ í‚¤ì›Œë“œ 5~20ê°œ ì¶”ì¶œ
- ì¤‘ë³µ ì œê±° + ëœë¤ ì…”í”Œ
- 0ê°œì¼ ê²½ìš°: ë°±ì—… í‚¤ì›Œë“œ(ENV ë˜ëŠ” íŒŒì¼)ë¡œ ëŒ€ì²´
- ê²°ê³¼ë¥¼ keywords.csv ì— ì €ì¥(ì²« ì¤„ = ì˜¤ëŠ˜ ì“¸ í‚¤ì›Œë“œ)

ì™¸ë¶€ ì˜ì¡´ì„±: requests, python-dotenv (requirements.txtì— í¬í•¨)
í™˜ê²½ë³€ìˆ˜:
  NAVER_CLIENT_ID, NAVER_CLIENT_SECRET (ì„ íƒ)
  NEWSAPI_KEY (ì„ íƒ)
  BACKUP_KEYWORDS (ì„ íƒ, ì½¤ë§ˆ êµ¬ë¶„)  ì˜ˆ: "ì•„ì´í° 16 ë°°í„°ë¦¬, ì¹´ì¹´ì˜¤í†¡ ë³´ì•ˆ ì„¤ì •, ..."
  KEYWORDS_CSV (ì„ íƒ, ê¸°ë³¸ê°’ "keywords.csv")
"""

import os
import re
import csv
import json
import time
import random
import logging
from urllib.parse import quote
from xml.etree import ElementTree as ET

import requests
from dotenv import load_dotenv

load_dotenv()

# ===== ì„¤ì • =====
KEYWORDS_CSV = os.getenv("KEYWORDS_CSV", "keywords.csv")
NAVER_CLIENT_ID = os.getenv("NAVER_CLIENT_ID") or ""
NAVER_CLIENT_SECRET = os.getenv("NAVER_CLIENT_SECRET") or ""
NEWSAPI_KEY = os.getenv("NEWSAPI_KEY") or ""

MIN_KEYS, MAX_KEYS = 5, 20
NGRAM_MIN, NGRAM_MAX = 2, 4
TIMEOUT = 15
RETRY = 3
BACKOFF = 2

# ê¸°ë³¸ ì§ˆì˜(í•œêµ­ì–´ ì¼ë°˜/IT/ìƒí™œ ì„ì–´ ë‹¤ì–‘ì„± í™•ë³´)
SEED_QUERIES = [
    "ì˜¤ëŠ˜ì˜ ë‰´ìŠ¤", "ì‹¤ì‹œê°„ ì´ìŠˆ", "í…Œí¬ ë‰´ìŠ¤", "ëª¨ë°”ì¼ ì†Œì‹",
    "ìƒí™œ ê¿€íŒ", "ì •ë¶€ ë°œí‘œ", "ê²½ì œ ë™í–¥", "ë¬¸í™” íŠ¸ë Œë“œ"
]

# ì •ì±…/ë¸Œëœë“œ ë¦¬ìŠ¤í¬ í‚¤ì›Œë“œ(í•„ìš” ì‹œ ì¶”ê°€)
BLOCK_WORDS = {
    "ì„±ê´€ê³„","í¬ë¥´ë…¸","ì•¼ë™","ë¶ˆë²•ì´¬ì˜","ìŒë€","ë…¸ê³¨","ê°•ê°„","ëª°ì¹´",
    "í˜ì˜¤","ì¦ì˜¤","í­ë ¥","ìí•´","í…ŒëŸ¬","ë§ˆì•½","ì´ê¸°","ì„ ë™","ê°€ì§œë‰´ìŠ¤"
}

SAFE_FALLBACKS = [
    "ìƒí™œ ì •ë³´ ëª¨ìŒ", "ì•Œëœ° ì†Œë¹„ íŒ", "ëª¨ë°”ì¼ ì„¤ì • ê°€ì´ë“œ",
    "ë¸”ë¡œê·¸ ìµœì í™” ë°©ë²•", "ì›Œë“œí”„ë ˆìŠ¤ ì´ë¯¸ì§€ ìµœì í™”"
]

STOPWORDS = {
    "ë‹¨ë…","ì†ë³´","ì´ìŠˆ","í˜„ì¥","ì¢…í•©","ì¸í„°ë·°","ì—…ë°ì´íŠ¸","ê¸°ì","ì‚¬ì§„","ì˜ìƒ","í¬í† ",
    "ë‹¨ì²´","ê³µì‹","ë°œí‘œ","ë…¼ë€","ê²°ê³¼","ì´ìœ ","ë³€í™”","ë¶„ì„","ì „ë§","ê´€ë ¨","ì˜¤ëŠ˜","ì–´ì œ",
    "ë‚´ì¼","ë°©ê¸ˆ","ì§€ê¸ˆ","í•´ë‹¹","ì£¼ìš”","íŠ¹ì§‘","ì¹¼ëŸ¼","ì‚¬ì„¤","ì˜¤í”¼ë‹ˆì–¸","ì „ë¬¸"
}

# ===== ë¡œê¹… =====
logging.basicConfig(level=logging.INFO, format="[%(levelname)s] %(message)s")
log = logging.getLogger("update_keywords")


def _is_allowed(kw: str) -> bool:
    t = (kw or "").lower()
    return not any(b in t for b in BLOCK_WORDS)


# ===== HTTP GET(ì¬ì‹œë„) =====
def http_get(url: str, params: dict | None = None, headers: dict | None = None) -> requests.Response | None:
    last_err = None
    for i in range(RETRY):
        try:
            resp = requests.get(url, params=params, headers=headers, timeout=TIMEOUT)
            if resp.status_code == 200:
                return resp
            last_err = f"HTTP {resp.status_code} {resp.text[:180]}"
            log.warning(f"GET failed {i+1}/{RETRY}: {last_err}")
        except requests.RequestException as e:
            last_err = repr(e)
            log.warning(f"GET error {i+1}/{RETRY}: {last_err}")
        time.sleep(BACKOFF ** i)
    log.error(f"GET failed: {url} ({last_err})")
    return None


# ===== ì†ŒìŠ¤ 1: Google News RSS (ì „ì²´ í”¼ë“œ) =====
def fetch_google_news_titles() -> list[str]:
    titles: list[str] = []
    try:
        url = "https://news.google.com/rss?hl=ko&gl=KR&ceid=KR:ko"
        resp = http_get(url)
        if not resp:
            return titles
        root = ET.fromstring(resp.content)
        for item in root.findall(".//item/title"):
            t = item.text or ""
            if t:
                titles.append(t)
    except Exception as e:
        log.warning(f"Google News RSS parse error: {e}")
    return titles


# ===== ì†ŒìŠ¤ 2: NewsAPI (top-headlines, KR) =====
def fetch_newsapi_titles() -> list[str]:
    titles: list[str] = []
    if not NEWSAPI_KEY:
        return titles
    url = "https://newsapi.org/v2/top-headlines"
    params = {"country": "kr", "pageSize": 50, "apiKey": NEWSAPI_KEY}
    resp = http_get(url, params=params)
    if not resp:
        return titles
    try:
        data = resp.json()
        for art in data.get("articles", []):
            t = art.get("title") or ""
            if t:
                titles.append(t)
    except Exception as e:
        log.warning(f"NewsAPI parse error: {e}")
    return titles


# ===== ì†ŒìŠ¤ 3: Naver Search API (ë‰´ìŠ¤, ë‹¤ì¤‘ ì§ˆì˜) =====
def fetch_naver_news_titles() -> list[str]:
    titles: list[str] = []
    if not (NAVER_CLIENT_ID and NAVER_CLIENT_SECRET):
        return titles
    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }
    queries = ["ì†ë³´", "ì˜¤ëŠ˜", "í˜„ì¥", "ë¸Œë¦¬í•‘", "ë°œí‘œ", "ì´ìŠˆ", "ë¶„ì„"]
    for q in queries:
        try:
            url = f"https://openapi.naver.com/v1/search/news.json?query={quote(q)}&display=30&sort=sim"
            resp = http_get(url, headers=headers)
            if not resp:
                continue
            data = resp.json()
            for it in data.get("items", []):
                t = it.get("title") or ""
                if t:
                    titles.append(t)
            time.sleep(0.2)  # ì†ë„ ì œí•œ ì—¬ìœ 
        except Exception as e:
            log.warning(f"Naver API parse fail ({q}): {e}")
    return titles


# ===== ì „ì²˜ë¦¬ & n-gram =====
def clean_title(t: str) -> str:
    if not t:
        return ""
    t = re.sub(r"<\/?b>", "", t)                      # ë„¤ì´ë²„ íƒœê·¸ ì œê±°
    t = re.sub(r"\[.*?\]|\(.*?\)|ã€.*?ã€‘|ã€ˆ.*?ã€‰|ã€Œ.*?ã€|ã€.*?ã€|<.*?>", " ", t)
    t = re.sub(r"[\"'â€œâ€â€˜â€™â€¢Â·â€¦~_=+^#@%&*|/:;]", " ", t)
    t = re.sub(r"[\u200b\u200c\u200d\ufeff]", "", t)
    t = re.sub(r"\s+", " ", t).strip()
    return t


def tokenize_ko(s: str) -> list[str]:
    toks = [w for w in re.split(r"\s+", s) if w]
    toks = [w for w in toks if w not in STOPWORDS and not w.isdigit()]
    toks = [w for w in toks if len(w) > 1]
    return toks


def ngrams(words: list[str], n: int) -> list[str]:
    return [" ".join(words[i:i+n]) for i in range(0, max(0, len(words)-n+1))]


def extract_candidates(titles: list[str]) -> list[str]:
    pool: set[str] = set()
    for t in titles:
        t = clean_title(t)
        if not t:
            continue
        toks = tokenize_ko(t)
        # 2~4 gram ì¡°í•©
        for n in range(NGRAM_MIN, NGRAM_MAX + 1):
            for g in ngrams(toks, n):
                if any(sw in g for sw in STOPWORDS):
                    continue
                if re.search(r"\b\d{1,4}\b", g):
                    # ì—°ë„/ìˆ«ìë§Œ ìˆëŠ” ì¡°í•©ì€ íŒ¨ìŠ¤
                    continue
                if 6 <= len(g) <= 32:
                    pool.add(g)
    return list(pool)


# ===== ë°±ì—… í‚¤ì›Œë“œ ë¡œë”© =====
def load_backup_keywords() -> list[str]:
    env_k = os.getenv("BACKUP_KEYWORDS", "").strip()
    if env_k:
        items = [x.strip() for x in env_k.split(",") if x.strip()]
        if items:
            return items

    if os.path.exists("backup_keywords.txt"):
        with open("backup_keywords.txt", "r", encoding="utf-8") as f:
            lines = [ln.strip() for ln in f if ln.strip()]
            if lines:
                return lines
    return []


def main():
    log.info("ğŸ” Collecting titles...")
    titles: list[str] = []

    # ì—¬ëŸ¬ ì†ŒìŠ¤ì—ì„œ ìˆ˜ì§‘
    titles += fetch_google_news_titles()
    titles += fetch_newsapi_titles()
    titles += fetch_naver_news_titles()

    # ì¤‘ë³µ ì œê±°
    titles = list(dict.fromkeys([t for t in titles if t]))
    log.info(f"Collected titles: {len(titles)}")

    candidates = extract_candidates(titles)
    random.shuffle(candidates)

    # í•œêµ­ì–´ í¬í•¨ + ê¸¸ì´/ë¸”ëŸ­ í•„í„° + ìµœëŒ€ì¹˜ ì»·
    filtered: list[str] = []
    seen = set()
    for s in candidates:
        if not (6 <= len(s) <= 32):
            continue
        if not re.search(r"[ê°€-í£]", s):
            continue
        if not _is_allowed(s):
            continue
        key = s.lower()
        if key in seen:
            continue
        seen.add(key)
        filtered.append(s)
        if len(filtered) >= MAX_KEYS:
            break

    # ìµœì € ê°œìˆ˜ ë³´ì¥ (ëª¨ìë¼ë©´ ì•ˆì „ í‚¤ì›Œë“œë¡œ ë³´ì¶©)
    if len(filtered) < MIN_KEYS:
        backup = load_backup_keywords() or SAFE_FALLBACKS
        random.shuffle(backup)
        for pick in backup:
            k = pick.lower()
            if k not in seen:
                filtered.append(pick)
                seen.add(k)
            if len(filtered) >= MIN_KEYS:
                break

    if not filtered:
        log.error("âŒ í‚¤ì›Œë“œ í›„ë³´ë¥¼ êµ¬ì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ë‚´ì¥ ì•ˆì „ í‚¤ì›Œë“œ ì‚¬ìš©.")
        filtered = SAFE_FALLBACKS[:MIN_KEYS]

    # ì˜¤ëŠ˜ì˜ ëœë¤ 1ê°œë¥¼ ë§¨ ìœ„ë¡œ (auto_wp_gpt.pyëŠ” ì²« ì¤„ì„ ì‚¬ìš©)
    random.shuffle(filtered)
    today = random.choice(filtered)
    filtered.remove(today)
    keywords = [today] + filtered

    # íŒŒì¼ ì €ì¥ (UTF-8, ê°œí–‰ ê³ ì •, BOM ì—†ìŒ)
    os.makedirs(os.path.dirname(KEYWORDS_CSV) or ".", exist_ok=True)
    with open(KEYWORDS_CSV, "w", encoding="utf-8", newline="") as f:
        w = csv.writer(f)
        for kw in keywords:
            w.writerow([kw])

    log.info(f"ğŸ“ keywords.csv updated. Count={len(keywords)} | First(today): {today}")


if __name__ == "__main__":
    main()